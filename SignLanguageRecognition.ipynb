{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IlwhV0z2nMr"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chprzdhe2nMw"
      },
      "source": [
        "# Assignment 4\n",
        "\n",
        "**DUE: Sunday November 21, 2021 11:59pm**\n",
        "\n",
        "Turn in the assignment via Canvas.\n",
        "\n",
        "To write legible answers you will need to be familiar with both [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and [Latex](https://www.latex-tutorial.com/tutorials/amsmath/)\n",
        "\n",
        "Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Runtime→→Restart runtime) and then run all cells (in the menubar, select Runtime→→Run All).\n",
        "\n",
        "Make sure you fill in any place that says \"YOUR CODE HERE\" or \"YOUR ANSWER HERE\", as well as your name below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jeWubXE2nMx"
      },
      "outputs": [],
      "source": [
        "NAME = \"Suneet Bhandari\"\n",
        "STUDENT_ID = \"1704322\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyRMGyp9Y9c3"
      },
      "source": [
        "## Gesture Recognition\n",
        "---\n",
        "**BEFORE YOU START: Change the runtime to GPU. From the \"Runtime\" dropdown menu in the toolbar above select \"change runtime type\". Then change :Hardware accelerator\" to GPU.**\n",
        "\n",
        "\n",
        "American Sign Language (ASL) is a complete, complex language that employs signs made by moving the hands combined with facial expressions and postures of the body. It is the primary language of many North Americans who are deaf and is one of several communication options used by people who are deaf or hard-of-hearing.\n",
        "\n",
        "The hand gestures representing English alphabet are shown below. In this question, you will focus on classifying these hand gesture images using convolutional neural networks. Specifically, given an image of a hand showing one of the letters, we want to detect which letter is being represented.\n",
        "\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1nRxq6yqDkmumUuePXfDx_5YgGl9vKXcj' width=\"300\">\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1U2O-ElzqES"
      },
      "source": [
        "Run the following code cell to download the training and test data. It might take a while to download the zip file and extract it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iExiItBkdS8F",
        "outputId": "2cd761b3-4af3-4ca4-8955-dd1d67887544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace asl_alphabet_train_1000/A/A1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace asl_alphabet_train_1000/A/A10.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace asl_alphabet_train_1000/A/A100.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: none\n",
            "replace asl_alphabet_train_1000/A/A1000.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import io\n",
        "import zipfile\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = '11V_w6LMLhGcdTU-dX_mw_SZxpHmHNq-x'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('asl_alphabet_1000.zip')\n",
        "!unzip -q asl_alphabet_1000.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmzKWUH0sLSl"
      },
      "source": [
        "Now that you downloaded the data, you see a directory containing 26 subdirectories that contain the hand gesture images. Notice that the subdirectories are named after the classes. Each of the subdirectories contains 1000 RBG images for its' respective class. Each RBG image has a height and width of $200\\times 200$. In Questions 1, 2, and 3, you will use use the tensorflow .image_dataset_from_directory() to create training and validation sets. The following cells implent a small tutorial for visualizing some training examples.\n",
        "\n",
        " Coding examples from adapted from: https://www.tensorflow.org/tutorials/load_data/images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WyRqjCotAMj"
      },
      "source": [
        "### Create a dataset\n",
        "Define some parameters for the loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5Im1justHf0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 32 # The batch size\n",
        "img_height = 200 # Image resize height\n",
        "img_width = 200 # Image resize width\n",
        "data_dir = \"asl_alphabet_train_1000\" # Data directory; you may need to change to location of asl_alphabet_train_1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nktS4ASUvEZ8"
      },
      "source": [
        "Use 80% of the images for training and 20% for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD9af5WsKzRd"
      },
      "outputs": [],
      "source": [
        "!ls asl_alphabet_train_1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz-CGbavvAPz"
      },
      "outputs": [],
      "source": [
        "# Create training dataset\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKc3DltZvh7X"
      },
      "outputs": [],
      "source": [
        "# Create validation dataset\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOWB_rhIv0Kh"
      },
      "source": [
        "Print the class names in the class_names attribute on these datasets. Notice that the class names are inferred from the subdirectory's names.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkPngCa8vwpQ"
      },
      "outputs": [],
      "source": [
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvhUcdsxwZsP"
      },
      "source": [
        "### Visualize the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMZiGFlSwmoO"
      },
      "source": [
        "Here are the first 9 images from the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dZeGJhawgjl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwgoEOoUwpKn"
      },
      "source": [
        "Here are the first 9 images from the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2gQ9DmcwswY"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in val_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMXuDljfxGoF"
      },
      "source": [
        "You can train a model using these datasets by passing them to model.fit (shown later in this tutorial). Let's retrieve one batch from the training datset and check the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuJMTfatxPo8"
      },
      "outputs": [],
      "source": [
        "for features_batch, labels_batch in train_ds:\n",
        "  print(features_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break\n",
        "count = 0\n",
        "count = len(train_ds)\n",
        "print(count)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6wzOwWJxcMt"
      },
      "source": [
        "features_batch is a single single batch from the training dataset. It contains $32$ $height=200$ by, $width=200$ by, $channel=3$ images from the training dataset. The size of the last dimension is 3, and contains the RGB values of the pixels. The labels are simply of size (32,). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AD1OSA8Qcmp"
      },
      "source": [
        "##  Question 1 - Fully-Connected Neural Network\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6CmHf0lMWxS"
      },
      "source": [
        "###Part A) Understanding and Processing the Data (10 points)\n",
        "\n",
        "Now that you downloaded the data, you should see a folder containing the images in their respective subdirectories. Complete the following steps (you may reuse the code from the tutorial):\n",
        "\n",
        "1) read in the training and test data.\n",
        "\n",
        "2) make sure that all of your images are of size $200\\times 200$. If not, scale them appropriately.\n",
        "\n",
        "3) rescale the pixel values of the training and test images from [0,255] to [0,1]. <br> Hint: tf.keras.layers.experimental.preprocessing.Rescaling(1./255) is recommended, see example at https://www.tensorflow.org/tutorials/load_data/images)\n",
        "\n",
        "4) Ensure that your target values (classes) are stored appropriately. You must have 26 classes for 'A-Z'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JiJtxu21_OK"
      },
      "outputs": [],
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch, labels_batch = next(iter(normalized_ds))\n",
        "normalized_vs = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch2, labels_batch2 = next(iter(normalized_vs))\n",
        "first_image = image_batch[0]\n",
        "first_image2 = image_batch2[0]\n",
        "# Notice the pixel values are now in `[0,1]`.\n",
        "print(np.min(first_image), np.max(first_image))\n",
        "print(np.min(first_image2), np.max(first_image2))\n",
        "#for features_batch, labels_batch in train_ds:\n",
        "  #print(features_batch.shape)\n",
        "  #print(labels_batch.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZB2MkoFWEeg"
      },
      "source": [
        "###Part B) Building a Fully-Connected Neural Network (10 points)\n",
        "Now that the dataset is downloaded, let's see what happens when we try to allocate a fully-connected neural network with a flatten layer, two hidden layers and an output layer to take in the $200 \\times 200 \\times 3$ images. Recall that our batch size is still 32. Don't spend too much time on this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhM8pnUPRpFs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Input, Dense # only use these layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import * # you can use any optimizer\n",
        "\n",
        "# Create the network\n",
        "\n",
        "def build_model2(): \n",
        "  model = Sequential() \n",
        "  model.add(Flatten(input_dim=200*200*3, batch_size = 32))\n",
        "  model.add(Dense(units = 32, activation='relu')) \n",
        "  model.add(Dense(units = 32, activation='relu')) \n",
        "  model.add(Dense(units = len(class_names), activation='softmax'))\n",
        "  return model\n",
        "\n",
        "model = build_model2()\n",
        "# Build model \n",
        "\n",
        "\n",
        "# Get model summary\n",
        "### YOUR CODE HERE ###\n",
        "#model.build(None, 200,200,3)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfpxX567SSg_"
      },
      "source": [
        "Assuming the code is correct, did you get any errors upon running the cell? If so, why do you think this error occured? Also, how many parameters does the above model have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGjNlVf7Svja"
      },
      "source": [
        " There were some errors initially, saying I the model was not being build and the network inputs were wrong. I fixed this issue with simple syntax fixes. Also by inputing the dimensions and the batch size in the model. This model has 3,841,946 parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxlYO3Xj2qnv"
      },
      "source": [
        "##  Question 2 - Convolutional Neural Networks\n",
        "---\n",
        "You have seen the shortcomings of using a fully-connected neural network for image recognition tasks. You will now build a convolutional network. For the rest of this assignment, we are not going to give you any starter code. You are welcome to use any code from previous class exercises, section handouts, and lectures. You may reuse your training and validation sets that you created in Question 1. You should also write your own code.\n",
        "\n",
        "You may use the TensorFlow documentation freely. You might also find online tutorials helpful. However, all code that you submit must be your own.\n",
        "\n",
        "Make sure that your code is vectorized, and does not contain obvious inefficiencies (for example, unnecessary for loops). Ensure enough comments are included in the code so that your TA can understand what you are doing. It is your responsibility to show that you understand what you write.\n",
        "\n",
        "Follow the steps below to show your work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG3jg6st2zX3"
      },
      "source": [
        "#### Part A) Building the Network (15 points)\n",
        "Build a convolutional neural network model that takes the ($200\\times 200 \\times 3$ RGB) image as input, and predicts the letter.\n",
        "\n",
        "Explain your choice of the architecture: how many layers did you choose? What types of layers did you use? Did you use dropout or normalization layers? What about other decisions like activation functions, kernel size, stride, and padding? Lastly, how many parameters does your model have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYjqTkMf2zX4"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 16,      \n",
        "                 kernel_size = (3, 3), \n",
        "                 padding = 'Same',\n",
        "                 activation = 'relu', \n",
        "                 input_shape = (200, 200, 3)))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters = 32,      \n",
        "                 kernel_size = (3, 3), \n",
        "                 padding = 'Same',\n",
        "                 activation = 'relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(200, activation = 'relu'))  \n",
        "\n",
        "model.add(Dense(26, activation = \"softmax\")) \n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS7-DF74MEyr"
      },
      "source": [
        "Added multiple layers to the model with the size of 200x200x3 because that is how big each image is. \n",
        "Last layer added is 26 because of the number of classes there are. Kernel size is 3,3 due to there being 3 layers and making it easier to analyze the data. Padding to make sure the data is read efficiently and nothing is missed. Total number of parameters is 16,010,578."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7arv3llO5e6u"
      },
      "source": [
        "#### Part B) Training the Network (15 points)\n",
        "Write code that trains your CNN given the training data (check the dataset tutorial to see how to use .fit with your custom dataset). Your training code should make it easy to tweak the usual hyperparameters, like batch size, learning rate, and the model object itself. Make sure that you are checkpointing your models from time to time (the frequency is up to you). Explain your choice of loss function and optimizer.\n",
        "\n",
        "Plot the training curve as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV1ntQiW5vRf"
      },
      "outputs": [],
      "source": [
        "\n",
        "batchsize = 32\n",
        "# Each epoch goes through the entire training set once\n",
        "epochs = 1 \n",
        "\n",
        "model.compile(optimizer = Adagrad(lr = 0.001),\n",
        "              loss = 'sparse_categorical_crossentropy', \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(train_ds,validation_data=val_ds,epochs = epochs, batch_size = batchsize,verbose = 1)\n",
        "#model.fit(train_ds,validation_data=val_ds,epochs = 50)\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUFE8Akq5x2M"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXdVut4462qR"
      },
      "source": [
        "#### Part C) Hyperparameter Search (15 points)\n",
        "\n",
        "1. List 3 hyperparameters that you think are most worth tuning. Choose at least one hyperparameter related to the model architecture.\n",
        "\n",
        "2. Tune the hyperparameters you listed previously, trying as many values as you need to until you feel satisfied that you are getting a good model. Plot the training curve of at least 4 different hyperparameter settings.\n",
        "\n",
        "3. Choose the best model out of all the ones that you have trained. Justify your choice.\n",
        "\n",
        "4. Report the test accuracy of your best model. You should only do this step once.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZTglQ5SXoAZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "batchsize = 26\n",
        "# Each epoch goes through the entire training set once\n",
        "epochs = 50  \n",
        "\n",
        "model.compile(optimizer = Adam(lr = 3e-4),\n",
        "              loss = 'sparse_categorical_crossentropy', \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(train_ds,validation_data=val_ds,epochs = epochs, batch_size = batchsize,verbose = 1)\n",
        "#model.fit(train_ds,validation_data=val_ds,epochs = 50)\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlfZqKrW7YJu"
      },
      "source": [
        "3 hyperparameters that I think are most worth tuning are the amount of hidden layers, learning rate, and the optimizers used. I changed the batch size, the number of epochs the optimizer and the learning rate. I choose the second model since it produces a better output than the first one. the accuracy for the both validation and training data was much better. The best test accuracy I saw was 0.9696."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6t_qjLr7-uf"
      },
      "source": [
        "## Question 3 - Transfer Learning\n",
        "---\n",
        "For many image classification tasks, it is generally not a good idea to train a very large deep neural network model from scratch due to the enormous compute requirements and lack of sufficient amounts of training data.\n",
        "\n",
        "One of the better options is to try using an existing model that performs a similar task to the one you need to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed Transfer Learning. In this assignment, we will use Transfer Learning to extract features from the hand gesture images. Then, train a smaller network to use these features as input and classify the hand gestures.\n",
        "\n",
        "As you have learned from the CNN lecture, convolution layers extract various features from the images which get utilized by the fully-connected layers for correct classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHk-PPM8KfZW"
      },
      "source": [
        "Keras even has pretrained models built in for this purpose. \n",
        "\n",
        "#### Keras Pretrained Models\n",
        "        Xception\n",
        "        VGG16\n",
        "        VGG19\n",
        "        ResNet, ResNetV2, ResNeXt\n",
        "        InceptionV3\n",
        "        InceptionResNetV2\n",
        "        MobileNet\n",
        "        MobileNetV2\n",
        "        DenseNet\n",
        "        NASNet\n",
        "\n",
        "Usually one uses the layers of the pretrained model up to some point, and then creates some fully connected layers to learn the desired recognition task. The earlier layers are \"frozen\", and only the later layers need to be trained. We'll use VGG16, which was trained to recognize 1000 objects in ImageNet. What we're doing here for our classifier may be akin to killing a fly with a shotgun, but the same process can be used to recognize objects the original network couldn't (i.e., you could use this technique to train your computer to recognize family and friends)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7CNXqACKgQz"
      },
      "outputs": [],
      "source": [
        "# Some stuff we'll need...\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad, Adadelta, RMSprop \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Activation\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# Powerful deep learning module.\n",
        "import tensorflow as tf\n",
        "\n",
        "# For dealing with data.\n",
        "import numpy as np  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4FK6jBJKxcC"
      },
      "source": [
        "Creating this pretrained network is a one line command. Notice we specified that the \"top\" should not be included. We aren't classifying 1000 different categories like ImageNet, so we don't include that layer. We'll add our own layer more suited to the task at hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR7DziD7KqEX"
      },
      "outputs": [],
      "source": [
        "# Import the VGG16 trained neural network model, minus it's last (top) neuron layer.\n",
        "base_model = VGG16(weights = 'imagenet', \n",
        "                   include_top = False, \n",
        "                   input_shape = (200, 200, 3), \n",
        "                   pooling = None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6HReXu8K3wh"
      },
      "source": [
        "Let's take a look at this pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPhnajBAKyKm"
      },
      "outputs": [],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEeQ9WXTLA6i"
      },
      "source": [
        "Please do realize, this may be overkill for our toy recognition task. One could use this network with some layers (as we're about to add) to recognize 100 dog breeds or to recognize all your friends. If you wanted to recognize 100 dog breeds, you would use a final 100 neuron softmax for the final layer. We'll need a final softmax layer as before. First let's freeze all these pretrained weights. They are fine as they are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTziRl8BK64Y"
      },
      "outputs": [],
      "source": [
        "# This freezes the weights of our VGG16 pretrained model.\n",
        "for layer in base_model.layers:  \n",
        "    print(layer)\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhTrIRPbLMCB"
      },
      "source": [
        "### Part A) Building the Classifier (10 points)\n",
        "Now let's just add a flatten layer, a trainable dense layer, and a final softmax layer to the network to complete the classifier model for our gesture recognition task. Use Keras' functional approach to building a network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YstHSchOLM7N"
      },
      "outputs": [],
      "source": [
        "# Now add layers to our pre-trained base model and add classification layers on top of it\n",
        "#x = ### YOUR CODE HERE ### \n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(units=60,activation='relu')(x)\n",
        "x = Dense(units=26,activation='softmax')(x)  \n",
        "\n",
        "# And now put this all together to create our new model.\n",
        "model = Model(inputs = base_model.input, outputs = x) \n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMnWikXkLczn"
      },
      "source": [
        "### Part B) Initializing Training Parameters (5 points)\n",
        "\n",
        "Compile the model using an appropriate loss function and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqWwKwCBLhyZ"
      },
      "outputs": [],
      "source": [
        "# Compile the model.\n",
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "# printing out a summary of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUWfx4vYtblN"
      },
      "source": [
        "### Part C) Training the Model (10 points)\n",
        "\n",
        "Train your new network, including any hyperparameter tuning. Plot the training curve of your best model only.\n",
        "\n",
        "As you can see here in the Keras docs:\n",
        "\n",
        "https://keras.io/api/applications/vgg/#vgg16-function\n",
        "\n",
        "that we are required to preprocess our image data in a specific way to use this pretrained model, so let's go ahead and do that first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX8s_zv6qM9G"
      },
      "outputs": [],
      "source": [
        "# Preprocess your input image data\n",
        "train_ds = train_ds.map(lambda x, y: (tf.keras.applications.vgg16.preprocess_input(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (tf.keras.applications.vgg16.preprocess_input(x), y))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9U0HITqLl-7"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "model_info = model.fit(train_ds,validation_data=val_ds,epochs = 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyezuk-2M1Gq"
      },
      "outputs": [],
      "source": [
        "# Plot the training curve\n",
        "\n",
        "# Data visualizaton.\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "import random as rn\n",
        "\n",
        "def plot_losses(model_info):\n",
        "    plt.plot(model_info.history[\"loss\"])\n",
        "    plt.plot(model_info.history[\"val_loss\"])\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'])\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracies(hist):\n",
        "    plt.plot(model_info.history['accuracy'])\n",
        "    plt.plot(model_info.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'])\n",
        "    plt.show()\n",
        "\n",
        "# Plot your losses and accuracies\n",
        "plot_losses(model_info)\n",
        "plot_accuracies(model_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y8v3jO4jIMU"
      },
      "source": [
        "### Part D) Your Best Classifier (10 points)\n",
        "\n",
        "Add on your own last layers to the pretrained model and train it on the training data (in the previous parts you could have only one flatten layer and one dense layer to do the classification). You can increase (or decrease) the number of nodes per layer, increase (or decrease) the number of layers, and add dropout if your model is overfitting, change the hyperparameters, change your optimizer, etc. Try to get the validation accuracy higher than what the previous transfer learning model was able to obtain, and try to minimize the amount of overfitting.\n",
        "\n",
        "Plot the classification accuracy for each epoch. Report the best test accuracy your model was able to achieve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXDiG-cdgsyZ"
      },
      "outputs": [],
      "source": [
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(units=200,activation='relu')(x)\n",
        "x = Dropout(rate=0.75)(x)\n",
        "x = Dense(units=50, activation='relu')(x)\n",
        "x = Dropout(rate=0.25)(x)\n",
        "output = Dense(units=26,activation='softmax')(x) \n",
        "\n",
        "# creating the new model\n",
        "new_model = Model(inputs = base_model.input, outputs = output) \n",
        "\n",
        "opt = Adam()\n",
        "new_model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "# printing out a summary of the model\n",
        "\n",
        "new_model_info = model.fit(train_ds,validation_data=val_ds,epochs = 50)\n",
        "\n",
        "plot_losses(new_model_info)\n",
        "plot_accuracies(new_model_info)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPZB5dK0DXIF"
      },
      "source": [
        "I flattened the data again and add 2 more hidden layers with dropout and it increased my accuracy. My highest test accuracy is .9950 which is very high in this case."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}