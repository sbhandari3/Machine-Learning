# -*- coding: utf-8 -*-


Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tX_iGGsQZZPH8pBsNxpubbW74gzAlzid
"""

b%matplotlib inline

"""# Assignment 3



Turn in the assignment via Canvas.

To write legible answers you will need to be familiar with both [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and [Latex](https://www.latex-tutorial.com/tutorials/amsmath/)

Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Runtime→→Restart runtime) and then run all cells (in the menubar, select Runtime→→Run All).

Make sure you fill in any place that says "YOUR CODE HERE" or "YOUR ANSWER HERE", as well as your name below:
"""

NAME = "Suneet Bhandari"
STUDENT_ID = "1704322"

"""## Question 1 - Logistic Regression
---
In this question, you will build a system for predicting patient deaths in the Intensive Care Unit (ICU) using the large [PhysioNet Computing in Cardiology Challenge 2012 dataset](https://physionet.org/content/challenge-2012/1.0.0/). For each patient in the dataset, demographic variables and time series of physiological variables were collected during their stay in the ICU. 

The risk prediction system you will build could in principle be used to flag patients as being at risk of death so that physicians could intervene and improve their outcome. To be confident about the impact of such a system, you would need to run an experiment. In this question, you will use a model in order to estimate the potential impact of the system.

The data you will be working with is all available from [PhysioNet](https://physionet.org/challenge/2012/). You will be looking at only the data in "Training set A". The patient data files are [https://archive.physionet.org/challenge/2012/set-a.zip](here), and the outcomes file is [https://archive.physionet.org/challenge/2012/Outcomes-a.txt](here).

#### Downloading and Reading in the Data

First, we download the physiological data and read in patient outcomes file (links above).

We want this data to be read from text files and assembled into a dataframe. To do so, please run the following code which will first define a function that reads a text file, and then runs that function on all the files and assembles the outputs into a single dataframe.

#### Downloading the data
"""

!wget https://archive.physionet.org/challenge/2012/set-a.zip -O ./set-a.zip
import zipfile
with zipfile.ZipFile('./set-a.zip', 'r') as zip_ref:
    zip_ref.extractall('./')

"""#### Reading in data"""

import pandas as pd
import numpy as np
import glob

def comp_patient(patdat, attrs):
    patdat[patdat == -1.0] = float('NaN')
    patdat_dict = {}
    for attr in attrs:
        patdat_dict[attr] = [patdat["Value"][patdat["Parameter"]==attr].mean(axis = 0)]
    return patdat_dict


attrs = ["Age", "Gender", "Height", "Weight", "Urine", 
         "HR", "Temp", "NIDiasABP", "SysABP", "DiasABP", "pH",
         "PaCO2", "PaO2", "Platelets", "MAP", "K", "Na", "FiO2", "GCS", "RecordID"]


full_dat  = pd.DataFrame(columns = attrs)

# Now let's take the list "all_pat_dat" and assemble it into a dataframe.
filenames = sorted(glob.glob("/content/set-a/*.txt"))
for filename in filenames:
    data = open(filename)
    patient_dat_full = pd.read_csv(data, delimiter=',')
    patient_dat = pd.DataFrame.from_dict(comp_patient(patient_dat_full, attrs))
    full_dat = full_dat.append(patient_dat)

outcome_dat = pd.read_csv("https://archive.physionet.org/challenge/2012/Outcomes-a.txt")
outcome_dat[outcome_dat == -1] = float('NaN') # set all -1 to NaNs
full_dat_out = full_dat.merge(outcome_dat, left_on='RecordID', right_on='RecordID')
full_dat_out.replace([np.inf, -np.inf], np.nan)

col_means = full_dat_out.mean()
# Set everything that's NaN to the mean of that column:

# Note: we do this for simplicity. Strictly speaking, you should
# only use the training set to compute the column means

for i in range(full_dat_out.shape[1]):
    mask = np.isnan(np.array(full_dat_out.iloc[:,i]))
    full_dat_out.iloc[mask, i] = col_means[i]

full_dat_out

"""### a) Understanding the Data (2 points)
Looking at the files and the code provided above, explain:


*   What is the size of your input data? how many parameters exist in the data?
*   Exaplain what the code provided above is trying to achieve.

The size of the input data is 4000 rows and there are 25 parameters/attributes
The code above is trying to read the data and sort it into all the different attributes. We are also calculating the means of all the columns. It is mainly transforming the data we have into a dataframe so we can use it for training

### b) Run Logistic Regression (9 points)


Divide your data into training, validation, and test sets (60-20-20). 

Use the features `HR`, `Gender`, `age`, `temperature`, `weight`, `height`, `PaO2`, and  `PaCO2`, and fit a logistic regression model to predict in-hospital death.
"""

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X = full_dat_out[['HR', 'Gender', 'Age', 'Temp', 'Weight', 'Height', 'PaO2', 'PaCO2']]
X = X.values
y = full_dat_out['In-hospital_death']
y = y.values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)

clf = LogisticRegression().fit(X_train, y_train)

"""### c) ROC Curve (10 points)

Write a function that, for a given threshold (prob. threshold), calculates both the False Positive Rate (proportion of non-deaths identified as deaths by the model) and True Positive Rate (proportion of deaths correctly identified as such by the model) for your regression model. 

For 100 threshold values equally spaced from 0 to 1, plot the True Positive Rate vs. the False Positive Rate. Use the validation set.

This plot is known as an ROC curve. 

"""

import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt
import matplotlib as mpl

def fpr_tpr_roc(pbt):
  tp, fp, fn, tn = 0, 0, 0, 0
  y_pred = (clf.predict_proba(X_val)[:, 1] >= pbt).astype(int)
  for i in range(len(y_pred)):
    if y_pred[i] == 1 and y_pred[i] == y_val[i]:
      tp += 1
    if y_pred[i] == 1 and y_pred[i] != y_val[i]:
      fp += 1
    if y_pred[i] == 0 and y_pred[i] != y_val[i]:
      fn += 1
    if y_pred[i] == 0 and y_pred[i] == y_val[i]:
      tn += 1      
  fpr = fp / (fp+tn)
  tpr = tp / (tp+fn)
  return fpr, tpr

t_a = np.arange(0,1,100) 
fprs, tprs = [],[]

for x in t_a:
  fpr, tpr = fpr_tpr_roc(x)
  fprs.append(fpr)
  tprs.append(tpr)

y_test_hat = clf.predict_proba(X_test)
y_val_hat = clf.predict_proba(X_val)

fpr_test, tpr_test, test_threshold = (metrics.roc_curve(y_test, y_test_hat[:,1]))
fpr_val, tpr_val, val_threshold = (metrics.roc_curve(y_val, y_val_hat[:,1]))
  #plot_roc_curve(clf,X_val,y_val)

probs = [0 for _ in range(len(y_val))]

plot_fpr, plot_tpr, _ = metrics.roc_curve(y_val, probs)
plt.plot(plot_fpr, plot_tpr)
plt.plot(fpr_val, tpr_val, label = 'roc curve')
plt.ylabel('TPR')
plt.xlabel('FPR')
plt.legend()
plt.show()

"""### d) Interpreting the ROC Curve (4 points)

Using the plot generated in Part (c), what is the False Positive Rate associated with correctly identifying 80% of patients at risk for death in the ICU? Why might a high false positive rate be appropriate in this setting? You can read the answer off the ROC curve plot.

The fpr while identifying correctly at 80% is a little above 60%. A high false positive rate might be appropriate in this setting because it will be easy to confirm the actual case after determining the result is a false positive, being that the fpr and tpr have to do with life or death.

### e) Modelling Doctors' Decision-Making (6 points)

For this part, produce a short report that answers all the questions below. Include code that produces the numbers that you need.

At the beginning of their shift, a doctor reviews their patients' charts, and decides what intervention is needed for each patient. In the following parts, we will be trying to improve this process. We will consider a simplified version of what is going on. Suppose that if the doctor intervenes correctly, the patient will not die; suppose that the doctor has 60 minutes to look through 25 patient charts; and suppose that the probability of missing the correct treatment if the doctor spends $t$ minutes on reviewing a file is

$$P(\textrm{fail}) = \exp(-t^2/100).$$


1. If the doctor reviews all the files, spends an equal amount of time on each chart, and there are 10 patients who will die without the correct intervention, how many patients are expected to die, if the doctor intervenes when they see that that's needed? What is the percentage of patients who are expected to die, out of 25?

2. Suppose now that the doctor is looking through all the patient charts in the validation set. They would have proportionately more time: $(N/25)\times 60$ minutes in total (where $N$ is the total number of patients in the set). How many patients would be expected to die, if the doctor intervenes correctly when they know they should do that?

3. Now, suppose that the doctor only reviews the files of patients for whom the model outputs a probability of greater than $20\%$. This would give the doctor more time to look through each file, but the doctor would never be able to intervene in the cases of patients form whom the output is $20\%$ or smaller. How many patients would be expected to die?
"""

print(np.count_nonzero(y_val))

est = clf.predict_proba(X_val)
count = 0
count2 = 0
count3 = 0
i = 0
for x in est:
  if x[1] > 0.2:
    count += 1

for x in est: 
  if x[1] > 0.2 and y_val[i] == 1:
    count2 += 1
  i += 1
i = 0

for x in est:
  if(y_val[i] == 1):
    count3 += 1
  i += 1

print(count)
print(count2)
print(count3)

"""1. the t in the P(fail) eqaution is equal to 60/25 which is to 2.4 
plugging t would be e^(-2.4^2 / 100) which is equal to 0.94402748291 which we can round to .944. the percentage of patients who would die out of 25 is 37.76 percent of patients since 9.44 patients would die, but also since we are talking about people it could be rounded to 10 and be 40 percent of patients.

2. 82 * .944 = 77.408 meaning that many people will die

3.

### f) Modelling Doctors' Decision-Making - Revisited (7 points)


In this Part, you will explore the policy implications of using our model in an understaffed hospital. 

Suppose that we are considering a policy of only reviewing the files of patients whose probability of death is above a threshold `thr`. Each chart would be given an equal amount of time, and the total amount of time will be $(N/25)\times 60$.

Using the model from previous part, plot the total number of expected deaths under the policy vs. the threshold. Using the plot, what is the best threshold to use that would minimize the number of deaths?

You should compute the expected number of deaths for the thresholds `np.arange(0, 1, 0.01)`.

Use the validation set.
"""

y_pred = (clf.predict_proba(X_val)[:, 1] >= pbt).astype(int)

ex_deaths = np.arange(0,1,0.01) 

ex_deaths = np.arange(0,1,0.01)

"""The threshold that would be most effective would be a threshold based on the amount time the limited staff has to see what each patient needs

### g) Modelling Doctors' Decision-Making - Testing (7 points)

On the test set, compare the total number of expected deaths under the best policy that was selected in Part (f) to reviewing each patient's file. In relative terms (i.e., as a percentage), how many lives would be saved, if the assumptions underlying our simulation are accurate?

The amount of lives that would be saved would be based on the amount of time given in the test data
"""



"""## Question 2 - Neural Networks
---

We will train a fully connected neural networks to recognize face images of cats and dogs.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers, losses
from tensorflow.keras.models import Model

data_dir = 'pets'
image_width = 32
image_height = 32
batch_size = 10

"""### a) Load the Pets Dataset (2 points)
There are 110 images of size $32\times32$. Run the following code block to import the data. Keep in mind that they will be downloaded from the internet, so it may take a while.
"""

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Download the data
downloaded = drive.CreateFile({'id':"1cA-dQ4tXusV0hQLK3JieI8GXQdvxNwbs"})
downloaded.GetContentFile('pets.npy')  
data = np.load("pets.npy",allow_pickle=True)

# Create X and y
X, y = (data[()]['X'], data[()]['y'])

# Get class names and create ID
class_names = ['cat', 'dog']
class_dict = {i:class_name for i,class_name in enumerate(class_names)}

# Plot some images
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for count, i in enumerate(range(0,110, 13)):
    ax = plt.subplot(3, 3, count + 1)
    plt.imshow(X[i],cmap='gray')
    plt.colorbar()
    plt.title(class_names[y[i]])
    plt.axis("off")

print(type(X)) 
print(type(y)) 
print(X.shape)
print(y.shape)
X.min()
X.max()

"""Answer the following questions:

1.) What are the data types for X and y?

2.) What are the shapes of X and y?

3.) What is the minimum and maximum value of X?

1. class 'numpy.ndarray'
2. X shape is (110,32,32), y shape is (110)
3. min is 0 and max is 255

### b) Data Preprocessing (3 points)
As you can see above, the pixel values are in the [0,255]. This is the normal range for images. Recall from the previous lectures and excercises that we need to normalize our data.

In order to normalize our data to [0,1] we use the equation:

$$x_{norm}=\frac{x-x_{min}}{x_{max}-x_{min}}$$

In our case we can assume that $x_{min}=0$ and $x_{max}=255$, this is a safe assumption since we are working with image data.

This means that for image data, if we want to normlize to [0,1] the equation simplifies to:

$$img_{norm}=\frac{img}{255}$$

Anytime you work with image data in any kind of model you will be normalizing with this equation. Unless the range you want to normalize is different. Sometimes you want to normalize between [-1,1], for that you would use a slightly different equation.

Normalize the X data using the above equation and save as train_images.
"""

train_images =  (X - X.min())/(X.max() - X.min())

"""If we show the image agian, you will see the values are all scaled correctly."""

# Plot images
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for count, i in enumerate(range(0,110, 13)):
    ax = plt.subplot(3, 3, count + 1)
    plt.imshow(train_images[i],cmap='gray')
    plt.colorbar()
    plt.title(class_names[y[i]])
    plt.axis("off")

"""What is the new minimum and maximum value of X?"""

X.max()

"""x min is 0 and x max is still 255 but in the pictures the max is 1

### c) Reshaping the Data (5 points)

Please reshpe training images to be 1024-dim (from $32 \times 32$). 

Hint: look into numpy.reshape().
"""

print(f'Before reshape, train_images shape: {train_images.shape}')

train_images = np.reshape(train_images,(110, 1024))

print(f'Before reshape, train_images shape: {train_images.shape}')

"""### d) Building a Simple Neural Network (15 points)
In the cell below, build a fully-connected feed-forward neural network with the following layers:
* **input layer** of shape 1024
* **hidden layer** with 25 neurons, and relu activation
* **output layer** with 1 neuron, and sigmoid activation
"""

from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.optimizers import *

# Build neural network
def build_model1():
    input_layer = Input(shape=(32*32))
    x = Dense(25, activation='relu')(input_layer)
    x = Dense(25, activation='relu')(x)
    x = Dense(1, activation='sigmoid')(x)
    return Model(input_layer, x)

# Show a summary of your model
model = build_model1()
model.summary()

"""Declare the optimizer using Stochastic Gradient Descient with learning rate of 0.001, weight decay of 1e-6 and momentum of 0.009. Compile your model using the 'binary_crossentropy' loss function and 'accuracy' as the metrics."""

# Declare optimizer
optimizer =  SGD(learning_rate=0.001, decay=1e-6, momentum=0.009)
model.compile(loss='binary_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])
# Compile model
# YOUR CODE HERE

"""Now we will train our model to learn pet faces using train_images and the 'y' labels. Our dataset is small so we will use 10% of the data as test set data. Train your model using the following parameters:

Fit your model using the train_images, 'y' labels, a batchsize of 10, and validation split of 0.1. Train for 500 epochs.

.fit documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Model
"""

y = y.reshape(-1,1)

# Call fit on your model passing in the X, y data above, train for 500 epochs
hist =  model.fit(train_images ,y, epochs=500, batch_size=10, validation_split=0.1)

def plot_losses(hist):
    plt.plot(hist.history['loss'])
    plt.plot(hist.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'])
    plt.show()
def plot_accuracies(hist):
    plt.plot(hist.history['accuracy'])
    plt.plot(hist.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'])
    plt.show()

# plot your losses and accuracies
plot_losses(hist)
plot_accuracies(hist)

"""For the input size, this one hidden layer neural network is probably too small. However, we can see that the model attempts to minimize the overall loss. Summarize your observations on the model's learning curves, model's overall classification accuracy on the validation set, and the least amount of loss on validation set?

For the training set the model the loss starts high but slowly goes away and the its accuracy ends up being near 1. For the validationg set the model's loss progressively goes up a its accuracy flattens out around 0.6. The least amount of loss is around 190 epochs and is 0.7596. This model could use an early stop as the accuracy flattens out around 200 epochs.

### e) Building a Second Neural Network (15 points)
Next, you will build a neural network with one hidden layer that is wider than the previous model's hidden layer. In the cell below, build a neural network with the following layers:
* **input layer** of shape 1024
* **hidden layer** with 100 neurons, and relu activation
* **output layer** with 1 neuron, and sigmoid activation
"""

# Build Neural network
def build_model1():
    input_layer = Input(shape=(1024))
    x = Dense(100, activation='relu')(input_layer)
    x = Dense(100, activation='relu')(x)
    x = Dense(1, activation='sigmoid')(x)
    return Model(input_layer, x)

model = build_model1()

# Show a summary of your model

model.summary()

"""Again, declare the optimizer using Stochastic Gradient Descient with learning rate of 0.001, weight decay of 1e-6 and momentum of 0.009. Compile your model using 'binary_crossentropy' loss function and 'accuracy' as the metrics."""

# Declare optimizer
optimizer =  SGD(learning_rate=0.001, decay=1e-6, momentum=0.009)
model.compile(loss='binary_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])
# Compile model
# YOUR CODE HERE

"""In this sell fit your model using the train images, y labels, a batchsize of 10, and validation split of 0.1. Train for 500 epochs."""

y = y.reshape(-1,1)

# Call fit on your model passing in the X, y data above, train for 500 epochs
hist =  model.fit(train_images ,y, epochs=500, batch_size=10, validation_split=0.1)

def plot_losses(hist):
    plt.plot(hist.history['loss'])
    plt.plot(hist.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'])
    plt.show()
def plot_accuracies(hist):
    plt.plot(hist.history['accuracy'])
    plt.plot(hist.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'])
    plt.show()

# Plot your losses and accuracies
plot_losses(hist)
plot_accuracies(hist)

"""Next, summarize this models results. Consider the following questions when answering. Remember that this model has a higher learning capacity than the previous model. 

1) How did this model compare to previous model? 

2) Did it outperform the previous models? 

3) What was your model's overall classification accuracy on the validation set and the least amount of loss on validation set?

This model has many more dips in the data for the validation set. This model is also more accurate for the majority of the plot. The models overall classification accuracy is around 0.7 because that is when there is the most data points. However this graph takes a lot longer converge and we do not know the exact point when it does. The  least val_loss is  0.4882 at the beginning of the model.

### f) Building a Third Neural Network (15 points)
Lastly, you will build a fully-connected neural network with two hidden layers. In the cell below, build a neural network with the following layers:
* **input layer** of shape 1024
* **hidden layer** with 100 neurons, and relu activation
* **hidden layer** with 25 neurons, and relu activation
* **output layer** with 1 neuron, and sigmoid activation
"""

from tensorflow.keras.layers import Input, Dense # only use these layers
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import * # you can use any optimizer

# Build Neural network
def build_model3():
    input_layer = Input(shape=(1024))
    x = Dense(100, activation='relu')(input_layer)
    x = Dense(25, activation='relu')(x)
    x = Dense(1, activation='sigmoid')(x)
    return Model(input_layer, x)

model = build_model3()

# Show a summary of your model
model.summary()

"""Since this model is bigger than the previous models, we will use a more powerful optimizer. Declare the optimizer as Adam with learning rate of 0.001. Compile your model using 'binary_crossentropy' loss function and 'accuracy' as the metrics."""

# Declare optimizer
optimizer =  Adam(learning_rate=0.001)
model.compile(loss='binary_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

# Compile model
# YOUR CODE HERE

"""In this sell fit your model using the train images, y labels, a batchsize of 10, and validation split of 0.1. Train for 500 epochs."""

y = y.reshape(-1,1)

# Call fit on your model passing in the X, y data above, train for 500 epochs
hist =  model.fit(train_images ,y, epochs=500, batch_size=10, validation_split=0.1)

def plot_losses(hist):
    plt.plot(hist.history['loss'])
    plt.plot(hist.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'])
    plt.show()
def plot_accuracies(hist):
    plt.plot(hist.history['accuracy'])
    plt.plot(hist.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'])
    plt.show()

# plot your losses and accuracies
plot_losses(hist)
plot_accuracies(hist)

"""Summarize this model's results. Consider the following when answering:

1) How did this model compare to previous models? 

2) Did it outperform the previous models? 

3) What was your model's overall classification accuracy on the validation set and the least amount of loss on validation set? 

4) Adam is a powerful optimizer, and may quickly converge to an a minima. Did this happen with this model?

This model us a little less accurate than the other models but converges much quicker for the validation set. It did not outperform the other models in the validation set but did for the training data. Early stopping for the epochs could be used here around 80 epochs which means its much more efficient just less accurate.Overall accuracy is a little about 0.5 and the amount of loss is val_loss: 0.2997. The converges is much faster as I stated earlier
"""