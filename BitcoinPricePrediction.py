# -*- coding: utf-8 -*-

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18c1VzL9nDcX3CW_MZtrSqfPAzzCZEpET
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

"""# Assignment 5

Turn in the assignment via Canvas.

To write legible answers you will need to be familiar with both [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and [Latex](https://www.latex-tutorial.com/tutorials/amsmath/)

Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Runtime→→Restart runtime) and then run all cells (in the menubar, select Runtime→→Run All).

Make sure you fill in any place that says "YOUR CODE HERE" or "YOUR ANSWER HERE", as well as your name below:
"""

NAME = "Suneet Bhandari"
STUDENT_ID = "1704322"

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers, losses
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model

"""## Question 1: Bitcoin Price Prediction
---
Bitcoin, amongst other assets like AMC and GME, has been all the rage this past year and reached an all time high of \$68,521 per bitcoin. Since reaching it's high, the price has dropped by nearly a half. Analysts continue to feed the frenzy by releasing price predictions that range from from \$500,000 to \$9,000 per bitcoin in the next year. You will make an Recurrent neural network model to gain some insight into price prediction. Yahoo! Finance is a trusted name in free financial information and has been with us since the internet's early years. You'll be using data gathered obtained from https://finance.yahoo.com/quote/BTC-USD/history/ to train your recurrent neural network.

Run the following code cell to download the training and test data. It might take a while to download the zip file and extract it.

Link to the raw data: [link](https://drive.google.com/file/d/1IvrXXUDB_kO4ydZb-lwtVF9BLccDtdJP/view?usp=sharing)
"""

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
import io
import zipfile
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
file_id = '1IvrXXUDB_kO4ydZb-lwtVF9BLccDtdJP'
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('BTC-USD.csv')

# Create pandas dataframe
data = pd.read_csv('BTC-USD.csv')

# Plot data.head()
data.head()




#print(data['High'] == 'NaN')

"""Let's plot the bitcoin price. First, we will make a plot of bitcoin price vs the days after September 15, 2014, the start date of this dataset. Day "0" indicates September 15, 2014."""

import matplotlib.pyplot as plt
import seaborn as sns # Used for plotting

g = sns.lineplot(x = np.linspace(1,2622,2622), y = data['High'].values.reshape(-1))

"""Next, we plot bitcoin vs. days. But this time there are dates added to the graph."""

import matplotlib.pyplot as plt
import seaborn as sns # Used for plotting

g = sns.lineplot(x = np.linspace(1,2622,2622), y = data['High'].values.reshape(-1))
#g.map(plt.plot, "a", "v", marker="o")
g.set(xticks=np.arange(0,2500,200))
g.set_xticklabels(rotation=30, labels = data['Date'][0::200])

"""### Part a) Data Preprocessing (5 points)
In this section you will preprocess the in order to train a recurrent neural network. We can see that there 5 columns, "Date", "Open", "High", "Low", "Close", "Adjusted Close", and "Volume". We will only use the High column.
"""

# Create a dataframe that only contains High Column.
# Hint: it may be helpful to now cast your pruned dataframe to a numpy array.
data = data.dropna()
data_high = data['High']

data_high.to_numpy()

#print(data_high)
data_high.head()

"""
Recall [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) from sklearn. Use it to scale the data for our analysis. """

### YOUR CODE HERE ###

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
data_high.values.reshape(-1,1)
data_normalized = scaler.fit_transform(data_high.values.reshape(-1,1))
print(data_normalized)
data_normalized = np.delete(data_normalized, np.where(data_normalized == None))

"""You will implement a simple autoregressive recurrent neural network using the standard tensorflow RNN architectures. An autoregressive model originates from the literature on time-series models where observations from the previous time-steps are used to predict the value at the current time step. To implemement an autoregressive model, we will simply augment the data so that "time_steps" number of previous days are fed to the model at the current time step in order to form a prediction. The function to augment the data is given to you:"""

def create_dataset(dataset, time_steps=1):
    """
    Generate a dataset where the time series X[n] contains the readings for the 'time_step' previous days 
    and y contains the price for today.
    args:
    dataset: numpy array, the data
    time_steps: the number of previous days to feed to RNN

    returns:
    tuple: a dataset with x[i] containing 'time_step' number of previous prices, target price for x[i]
    """
    dataX, dataY = [],[]
    for i in range(len(dataset)-time_steps-1):
        a = dataset[i:(i+time_steps)]
        dataX.append(a)
        dataY.append(dataset[i + time_steps])
    return np.array(dataX), np.array(dataY)

# Choose the number of time steps that the model "looks back"
time_steps = 5

# Produce your dataset based on the number of days the model could look back

X, y = create_dataset(data_normalized, time_steps)

# Check the shape of your dataset; should be (2622-time_steps-1, time_steps) and (2622-time_steps-1,)

print(X.shape, y.shape)

"""### Part b) Data Partitioning (5 points)
Split data into train and test sets. Use 80\% for training and 20\% for testing. **Note**: you need to split the data in time (the begining 80\% of the days from start date will be the training data and the remaining 20\% will be test data).
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = False)

"""For this dataset, you need to reshape the partitions for the model to be able to process them."""

# Reshape input to be [samples, time steps, features].
X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))
#y_train = np.reshape(y_train, (y_train.shape[0], 1))
#y_test = np.reshape(y_test, (y_test.shape[0], 1))
print(X_train.shape, y_train.shape)

#print(X_test)
#print(5)
#print(X_train)
#print(5)
#print(y_train)
print(5)
#print(y_test)

"""### Part c) RNN Model (15 points)
In this part you will create a model using an RNN layer (LSTM or GRU, unidirectional or bidirectional) and train it on your training data. You will also plot training and validation loss. Use mean squared error as your model's metric.

Compile your model and display the summary:
"""

import matplotlib.pyplot as plt
import pandas
import math
from keras.models import Sequential, Model
from keras.layers import Dense
from keras.layers import LSTM, Input, GRU
from sklearn.metrics import mean_squared_error
from tensorflow.keras.optimizers import Adam

# Build your model
input_layer = Input(shape=(1, time_steps))
x = LSTM(10)(input_layer) 
x = Dense(1, activation='relu')(x)
x = Dense(1, activation='sigmoid')(x)
model = Model(input_layer, x)

loss= tf.keras.losses.BinaryCrossentropy()
opt= Adam()
metrics = 'mean_squared_error'
model.summary()
model.compile(loss = loss, optimizer = opt, metrics = metrics)

batchsize = 256

epochs =  100

# Fit model
history = model.fit(X_train, y_train.reshape(-1,1), batch_size = batchsize, epochs = epochs, validation_split=0.2, shuffle = True)

# Plot the Model loss

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train', 'test'])
plt.show()

"""### Part d) More Advanced RNN Model (15 points)
In this part you will create an RNN model with the number of layers and architerure you prefer. Train it on your training data. You will also plot training and validation loss. Again, use mean squared error as your metric. In this part, you can try different models and use different hyper-parameters and report only the best one.

Compile your model and display the summary:
"""

# Build your model
input_layer = Input(shape=(1, time_steps))
x = GRU(20)(input_layer) 
x = Dense(10, activation='relu')(x)
x = Dense(1, activation='sigmoid')(x)
model = Model(input_layer, x)

loss= tf.keras.losses.BinaryCrossentropy()
opt= Adam()

metrics = 'mean_squared_error'
model.summary()
model.compile(loss = loss, optimizer = opt, metrics = metrics)

batchsize = 256

epochs =  100

# Fit model
history = model.fit(X_train, y_train, batch_size = batchsize, epochs = epochs, validation_split=0.2, shuffle = True)

# Plot the Model loss

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train', 'test'])
plt.show()

"""### Part e) Looking at the Predictions (10 points)
Now, Using the final (best) model you trained, show your model's performance on the test set. Plot the model's prediction for Bitcoin Price along with the actual test set prices. Lastly, note how your model's predictions change with your model's architecture and the number of days you "look back". Does your model perform better with more "look back days" or less. Did adding more layers help? Does your model use dropout or batchnormalization?

**Note:** Your model is trained on normalized data. Inorder to transform your model's predictions to the original price range you will likely need to use sklearn's inverse_transform (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)
"""

trainPredict = model.predict(X_train)
testPredict = model.predict(X_test)

trainPredict = scaler.inverse_transform(trainPredict)
testPredict = scaler.inverse_transform(testPredict)

plt.plot(trainPredict)
plt.plot(testPredict)
plt.show()

"""The number of days we look back the less accurate the model becomes. Adding more layers did help and I did not use dropout or batch normalization.

## Question 2: Reuters Topic Classification
---
We will use the [Reuters newswire](https://keras.io/api/datasets/reuters/) classification dataset, which has text paired with 46 topics as labels. You can see what these labels represent [here](https://martin-thoma.com/nlp-reuters/). You will analyze the text and classify the text into one of the 46 classes. Classes are the defined based on the following list (in the same order):
>```
['cocoa','grain','veg-oil','earn','acq','wheat','copper',
'housing','money-supply','coffee','sugar','trade','reserves',
'ship','cotton','carcass','crude','nat-gas','cpi','money-fx',
'interest','gnp','meal-feed','alum','oilseed','gold','tin',
'strategic-metal','livestock','retail','ipi','iron-steel',
'rubber','heat','jobs','lei','bop','zinc','orange',
'pet-chem','dlr','gas','silver','wpi','hog','lead']
```

### Part a) Processing the Data (5 points)
Load the data simply from Keras: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/reuters/load_data.
You also need to follow the RNN handout from the class exercise and prepare encoding and decoding functions and any other pre-processing you might find useful.

Print 5 examples of news along with their class label.
"""

# Load data
from tensorflow.keras.datasets import reuters
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_sequence_len = 1263

(X_train,y_train), (X_test,y_test)= reuters.load_data(maxlen=max_sequence_len, test_split = 0.2)
word_dict = reuters.get_word_index()
for i in range(50):
  for key, value in word_dict.items(): 
    if value == i:
        print('(', key, ',', value, ')', sep = '', end = ',')

print(len(word_dict))

word_dict = {k:(v+3) for k,v in word_dict.items()}
word_dict["<PAD>"] = 0
word_dict["<START>"] = 1
word_dict["<UNK>"] = 2
word_dict["<UNUSED>"] = 3

vocab_size = len(word_dict.keys())
print('Number of words in vocabulary: ', vocab_size)
for i in range(50):
  for key, value in word_dict.items(): 
    if value == i:
        print('(', key, ',', value, ')', sep = '', end = ',')
X_train = pad_sequences(X_train, maxlen=max_sequence_len)
X_test = pad_sequences(X_test, maxlen=max_sequence_len)



# Clean and pre-process the data



# Print sample data
print(X_train)
print(y_train)
print(y_test)
print(X_test)

"""### Part b) Data Partitioning (5 points)

Split data into train and test sets. Please use 80% for training and 20% for testing. Note that we want to have the same distribution of labels in the training and test set, so you can use stratified train-test split of Keras. See here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
"""

# ALREADY SPLIT IN INITIALL CELL PART A.

"""### Part c) Simple RNN Model (15 points)

Create a model using an RNN layer (LSTM or GRU, unidirectional or bidirectional) and train it on your training data. You will also plot training and validation loss and your accuracy metric.
"""

import tensorflow as tf
from tensorflow.keras.datasets import imdb
import matplotlib.pyplot as plt
import numpy as np
import string
import textwrap
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, Dense, Dropout, Input, LSTM, GRU
from tensorflow.keras.models import Model

input_layer = Input(shape=(max_sequence_len))
x = Embedding(vocab_size, 64)(input_layer)

x = LSTM(64)(x) 

x = Dense(64, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(32, activation='relu')(x)
x = Dropout(0.8)(x)

x = Dense(1, activation='sigmoid')(x)
sentiment_model = Model(input_layer, x)

"""Compile your model and display the summary:"""

sentiment_model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(2e-4),
              metrics=['accuracy'])
history = sentiment_model.fit(X_train, y_train.reshape(-1,1), batch_size=256, epochs=5, validation_split=0.2, shuffle=True)

# Plot the Model loss

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train', 'test'])
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['train', 'test'])
plt.ylim([0,1])
plt.show()

"""### Part d) More Advanced RNN Model (15 points)
In this part you will create an RNN model with the number of layers and architerure you prefer. Train it on your training data. You will also plot training and validation loss and your metric. In this part, you can try different models and use different hyper-parameters and report only the best one.

Compile your model and display the summary:
"""

# Build your model

input_layer = Input(shape=(max_sequence_len, 1))

x = GRU(64)(input_layer) 

x = Dense(64, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(32, activation='relu')(x)
x = Dropout(0.8)(x)

x = Dense(1, activation='sigmoid')(x)
model = Model(input_layer, x)
#input_layer = Input(shape=(max_sequence_len, 1))
#x = LSTM(10)(input_layer) 
#x = Dense(1, activation='linear')(x)
#x = Dense(1, activation='sigmoid')(x)
#model = Model(input_layer, x)

loss= tf.keras.losses.BinaryCrossentropy()
opt= tf.keras.optimizers.Adam(2e-4)

metrics = ['accuracy']



model.compile(loss=loss,
              optimizer=opt,
              metrics=metrics)

model.summary()

batchsize = 200
epochs =  15

# Fit model

history = model.fit(X_train, y_train, batch_size=batchsize, epochs=epochs, validation_split=0.2, shuffle=True)

# Plot the Model loss

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train', 'test'])
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['train', 'test'])
plt.ylim([0,1])
plt.show()

"""### Part e) Looking at the Predictions (10 points)

Now, Using the final (best) model you trained, show your model's performance on the test set.
Calculate and display the prediction accuracy for all of the 46 different classes. 
"""

trainPredict = model.predict(X_train)
testPredict = model.predict(X_test)



plt.plot(trainPredict)
plt.plot(testPredict)
plt.show()